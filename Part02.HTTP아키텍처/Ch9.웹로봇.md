# Ch9.웹로봇

> 웹 로봇

```
사람과의 상호작용 없이 연속된 웹 트랝잭션들을 자동으로 수행하는 소프트웨어 프로그램
```



#### 9.1 크롤로와 크롤링

> 웹 크롤러

```
- 재귀적으로 반복하여 웹을 순회하는 로봇
- 끌어온 문서는 검색 가능한 DB로 만들어서 특정 단어를 포함한 문서를 찾을 수 있게 함
```



##### 9.1.1 어디에서 시작하는가:' 루트 집합'

> 루트 집합(root set)

```
- 크롤러가 방문을 시작하는 모든 페이지를 도달 할 수 있는 URL의 초기 집합
- 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 잘 알려지지 않은 페이지 들의 목록으로 구성
```



##### 9.1.2 링크 추출과 상대 링크 정상화

```
- 크롤러는 검색한 페이지의 URL 링크를 파싱하여 크롤링할 페이지들의 목록에 추가하고 새 링크를 발견함에 따라 확장
- 상대 링크를 절대 링크로 변환할 필요 있음
```



##### 9.1.3 순환 피하기

```
- 순환은 로봇을 함정에 빠뜨려 멈추게 하거나 진행을 느려지게 하기 때문에 루프나 순환에 빠지지 않도록 어디를 방문했는지 알아야함
```



##### 9.1.4 루프와 중복

> 순환의 해로움

```
- 같은 페이지를 반복적으로 가져와 네트위크 대역폭을 다 차지할 수 있음
- 웹 서버에 부담. 웹 사이트를 압박하여 사이트에 접근 못하게 막을 수 있음
- 중복된 페이지 반환하는 검색엔진이 될 수 있음
```



##### 9.1.5 빵 부스러기의 흔적

```
- 방문한 곳을 지속적으로 추적하는 것은 어렵기 때문에 검색 트리나 해시 테이블이 필요할 것
```

###### 1. 트리와 해시 테이블

```
- 검색 트리나 해시 테이블을 사용하는 소프트웨어 자료구조
```

###### 2. 느슨한 존재 비트맵

````
- 각 URL이 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 존재 비트(Presence bit)를 갖음
- 존재 비트의 유무로 크롤링 확인
````

###### 3. 체크포인트

````
- 갑작스러운 중단을 대비하여 방문한 URL 목록이 디스크에 저장되었는지 확인
````

###### 4.파티셔닝

```
- 로봇들이 동시에 일하는 농장(Farm) 이용
- 각 로봇은 특정 부분을 할당하여 서로 도와 웹 크롤링을 함
```



##### 9.1.6 별칭(alias)과 로봇 순환

```
- URL의 별칭으로 서로 달라 보이더라도 같은 리소스를 가리킬 수 있음
```



##### 9.1.7 URL 정규화 하기

```
- URL을 표준 형식으로 정규화하여 같은 리소스를 가리키는 것들을 미리 제거
- 정규화된 형식으로 변환 할 수 있는 것
  (포트명 추가, %x대응 문자 변환, #태크 제거 등)
- 웹서버에 대한 지식이 필요한 것들이 있음
  (대소문자 구분, 디렉터리 설정, 물리적 참조와 가상 호스팅 설정 등)
```



##### 9.1.8 파일 시스템 링크 순환

>심벌릭 링크(subdir)

```
- 끝없이 깊어지는 디렉터리 계층을 만들 수 있음
- 교묘한 종류의 순환을 유발할 수 있음
- 서버 관리자가 실수로 만들지만 함정에 빠뜨리기 위해 악의적으로 만들기도함
```



##### 9.1.9 동적 가상 웹 공간

```
- 의도적으로 복잡한 크롤러 루프를 만들수 있음
- 가상의 URL로 요청을 받으면 새로운 가상 URL을 갖고 있는 새 HTML 페이지를 날조하여 만듬
- 이는 URL과 HTML이 매번 달라 보일 수 있기에 로봇이 순환을 감지하기 어려움
- 웹 마스터도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만들 수 있음
```



##### 9.1.10 루프와 중복 피하기

```
- 로봇은 순환을 피하기 위해 휴리스틱의 집합이 필요
- 다만 의심스러워 보이지만 유효한 콘텐츠를 걸러 내는 손실이 발생할 수 있음
```

###### 1. URL 정규화

```
- 중복된 URL이 생기는 것을 일부 회피
```

###### 2. 너비 우선 크롤링

```
- 너비 우선으로 스케줄링하여 함정을 건들여도 순환 페이지를 받기 전에 다른 웹 사이트를 받음
```

###### 3. 스로틀링

```
- 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한
- 서버에 대한 접근 횟수와 중복의 총 횟수를 제한
```

###### 4. URL 크기 제한

```
- 일정 길이를 넘는 URL의 크롤링을 거부
- 이 기법으로 가져오지 못하는 콘텐츠가 존재
- 제한이 되면 요청 URL이 특정 크기에 도달 할 때마다 에러 로그를 남겨서 어떤 일이 벌어지는지 감시하는 신호를 제공할 수 있음
```

###### 5. URL/사이트 블랙리스트

````
- 문제를 일으키는 사이트나 URL이 발견될 때마다 블랙리스트에 추가
````

###### 6. 패턴 발견

```
- 심벌릭 링크를 통한 순환과 비슷한 오설정들은 일정한 패턴을 따르는 경향이 있음
- 반복되는 구성요소를 갖고 있는 URL의 크롤링 거절
```

###### 7. 콘텐츠 지문(fingerprint)

```
- 페이지의 콘텐츠에서 몇바이트를 얻어내어 페이지 내용의 간략한 표현인 체크섬(checksum) 계산
- 이전에 보았던 체크섬을 가진 페이지면 크롤링을 하지 않음
- 동적, 커스터마이징 한 것은 중복 감지를 방해할 수 있음
```

###### 8. 사람의 모니터링

```
- 로봇의 진행상황을 모니터링하여 특이한 일이 일어나면 즉각 인지할수 있께 진단과 로깅을 포함하도록 설계해야 함
```





#### 9.2 로봇의 HTTP

```
- 로봇은 찾는 콘텐츠를 요청하기 위해 필요한 HTTP를 최고한으로 구현하려 함
- 그렇기 때문에 많은 로봇이 요구사항이 적은 HTTP/1.0 요청을 보냄
```



##### 9.2.1 요청 헤더 식별하기

> 신원 식별 헤더 구현

```
- User-Agent: 서버에게 요청을 만든 로봇의 이름
- From: 로봇의 사용자/관리자의 이메일 주소를 제공
- Accept: 서버에게 보내도 되는 미디어 타입 언급
- Referer: 현재의 요청 URL을 포함한 문서의 URL 제공
```



##### 9.2.2 가상 호스팅

```
- 헤더가 없으면 잘못된 콘텐츠를 찾을 수 있기 때문에 HTTP/1.1은 Host 헤더 사용을 요구
- 기본적으로 하나의 서버는 하나의 사이트를 운영하지만 2개의 사이트를 운영하는 서버에 헤더 없이 요구하게 되면 잘못된 콘텐츠를 받을 수 있음
```



##### 9.2.3 조건부 요청

```
- 인터넷 검색엔진 로봇의 경우, 변경되었을 때만 콘텐츠를 가져오게 업데이트된 것만 알아보는 조건부 HTTP 요청을 구현
```



##### 9.2.4 응답 다루기

###### 1. 상태 코드

```
- 최소한 일반적인 상태코드나 예상할 수 있는 상태 코드를 다를 수 있어야 함
- 명시적으로 이해할 수 없는 상태 코드는 그 코드가 속한 분류에 근거하여 다루어야함
```

###### 2. 엔터티

```
- HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터디 자체의 정보를 찾을 수 있음
- 메타 http-equiv 태그가 HEAD 섹션에 없다면 다른 영역에 탐색할 수 있어야 함
```



##### 9.2.5 User-Agent 타기팅

```
- 로봇들로부터 요청을 예상하고 여러 기능을 지원할 수 있도록 브라우저 종류를 감지하여 콘텐츠를 최적화함
- 풍부한 기능을 갖추지 못한 브라우저나 로봇 등을 대응하기 위해 유연한 페이지를 개발해야 함
```





#### 9.3 부적절하게 동적하는 로봇들

###### 1. 폭주하는 로봇

```
- 논리적인 에러를 갖고 있꺼나 순환에 빠지면 웹 서버에 극심한 부하를 안겨줄 수 있음
- 서버에 과부하를 유발하여 누구도 서비스를 못받게 만드는 일도 발생
```

###### 2. 오래된 URL

```
- 오래된 URL 목록으로 인해 존재하지 않은 URL에 대한 요청이 발생
- 그러 인해 에러 로그로 채워져 부하로 인해 웹 서버 요청의 수용 능력이 감소
```

###### 3. 길고 잘못된 URL

```
- 순환이나 프로그래밍상의 오류로 의미 없는 URL을 요청할 수 있음
- 처리 능력에 영향을 주고 접근 로그를 어지럽게 채우고 고정을 일으킬 수 있음
```

###### 4. 호기심이 지나친 로봇

```
- 사적인 데이터에 대한 URL로 검색엔진이나 기타 애플리케이션을 통해 접근할 수 있도록 만듬
- 민감한 데이터를 로봇이 검색할 수 있음
- 악의적인 검색엔진과 아카이브 사용자들은 콘텐츠를 찾기 위해 대규모 웹 크롤러의 능력을 이용하는 것으로 알려져 있음
```

###### 5. 동적 게이트웨이 접근

```
- 게이트웨이 애플리케이션의 콘텐츠에 대한 URL을 요청할 수 있고 처리 비용이 많이 듦
```





#### 9.4 로봇 차단하기

> robots.txt = Robots Exclusion Standard

```
- 로봇의 동작을 제어할 수 있는 메커니즘 제공
- 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보
- 로봇은 페이지를 요청하기 전에 robots.txt 파일을 검사하여 권한을 확인
```



##### 9.4.1  로봇 차단 표준

```
- 임시방편으로 마련된 표준으로 표준의 부분집합을 제각각 구현 하고 있음
- v0.0 v1.0 v2.0으로 세 가지 버전 존재
- v2.0은 복잡하여 널리 채택 못됨
- 위 책은 주로 v0.0, v1.0이 채택되고 v0.0이 완전 호환되는 v1.0 표준애 초점
```



##### 9.4.2 웹 사이트와 robots.txt 파일들

```
- 호스트 명과 포트번호에 의해 정의되는 웹 사이트의 robots.txt는 하나만 존재
- 가상 호스팅되어있다면 다른 robots.txt 있을 수 있음
- robots.txt가 반드시 파일 시스템에 존재할 필요는 없음
```

###### robots.txt 가져오기

```
- robots.txt가 존재하면 서버는 그 파일을  text/plain 본문으로 반환
- 서버가 404 에러 상태코드로 응답하면 접근을 제한하지 않은 것으로 간주
- 로봇은 사이트 관리자가 로봇의 접근을 추적할 수 있도록 헤더를 통해 신원정보, 연락처 제공해야함
```

###### 응답코드

```
- 로봇은 웹 사이트의 robots.txt 유무와 상관없이 robots.txt를 찾아봄
- 로봇은 robots.txt 검색 결과에 따라 다르게 동작
```



##### 9.4.3 robots.txt 파일 포멧

```
- robots.txt파일의 각 줄은 빈 줄, 주석 줄, 규칙 줄로 구성됨
- 규칙 줄은 HTTP 헤더 처럼 생겼고 패턴 매칭을 위해 사용 <필드>:<값>
- robots.txt는 레코드로 구분되고 로봇별로 다른 차단 규칙 적용 가능
- 각 레코드는 빈줄이나 파일 끝(end-of-file) 문자로 끝남
```

```
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```



###### User-Agent 줄

```
User-Agent : <robot name>
- 로봇 이름이 대소문자 구분없이 자신 이름의 부분 문자열이 될 수 잇는 레코드들 중 첫번째
User-Agent : *
- 로봇 이름이 '*'인 레코드들 중 첫번 째
```

###### Disallow와 Allow 줄들

```
- User-Agent 줄들 바로 다음에 옴
- 어떤 URL 경로가 명시적으로 금지되고 허용되는지 기술
```

###### Disallow/Allow 접두 매칭(prefix matching)

```
- 경로의 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야 함
  /tmp --> /tmp , /tmpfile.html
- 빈 문자열은 모든 문자열에 매치
  (빈 문자열) --> README.md
- 이스케이핑된 문자(%XX)은 비교 전에 원래대로 복원 (단, 빗금/을 의미하는 %2F는 예외)
  /%7efred/hi.html --> /~fred/hi.html
```



##### 9.4.4 그 외에 알아둘 점

```
- 로봇은 자신이 이해 못하는 필드는 무시
- 한 줄을 여러 줄로 나눠 적는 것 허용 X
- v0.0는 Allow줄을 지원하지 않기 않아서 무시되기 때문에 탐색하지 않을 수 있음
```

 

##### 9.4.5 robots.txt의 캐싱과 만료

```
- 주기적으로 robots.txt를 가져와 결과를 캐시
- 캐시된 사본은 만료될 때 까지 로봇에 의해 사용
- 캐싱은 Cache-Control과 expires 헤더를 통해 제어
```



##### 9.4.6 로봇 차단 펄 코드

```
- robots.txt 파일과 상호작용하는 공개된 펄(Perl) 라이브러리가 존재
- URL에 대한 접근이 금지되어 있는지 확인할 수 있는 메서드 제공
```



##### 9.4.7 HTML 로봇 제어 META 태그

```
- HTML 페이지 저자는 HTML 문서에 직접 로봇 제어 태그를 추가할 수 있음
- 로봇 차단 태그는 HTML META 태그를 이용
```

###### 로봇 META 지시자

```
지시들이 충돌하거나 중복되게 해서는 안됨!!
- NOINDEX : 로봇에게 이 페이지를 처리하지 말고 무시
- NOFOLLOW : 링크한 페이지를 크롤링 x
- INDEX : 페이지의 콘텐츠 인덱싱 가능
- FOLLOW : 크롤링 가능
- NOARCHIVE : 페이지의 캐시를 위한 로컨 사본 만들면 안돼!
- ALL :  == INDEX,FOLLOW
- NONE : == NOINDEX, NOFOLLOW
```





#### 9.5 로봇 에티켓

```
1. 신원 식별 :  로봇의 신원을 밝히라
2. 동작 : 긴장하라
3. 스스로 제한하라 : URL을 필터링하라
4. 루프와 중복을 견뎌내기, 그리고 그 외의 문제들 : 모든 응답코드 다루기
5. 확장성 : 공간 이해하기
6. 신뢰성 : 철저하게 테스트하라
7. 소통 : 준비하라
```





#### 9.6 검색엔진

```
- 웹 사용자들의 시적점인 동시에 관심 있는 정보를 찾을 수 있도록 도와주는 서비스
```



##### 9.6.1 넓게 생각하라

```
- 초기에는 문서의 위치를 알아내는 단순한 DB였지만 급격한 성장과 진화를 하면서 복잡해짐
- 검색엔진은 수입억 개의 웹페이지들을 검색하기 위해 복잡한 크롤러를 사용
```



##### 9.6.2 현대적인 검색엔진의 아키텍처

```
- 검색엔진 크롤러는 웹페이지를 수집하여 풀 텍스트 색인에 추가
- 사용자는 웹 검색 게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보냄
```



##### 9.6.3 풀 텍스트 색인

```
- 단어 하나를 입력 받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 DB
```



##### 9.6.4 질의 보내기

```
- HTML 폼을 사용자가 채워 넣고 브라우저가 폼을 HTTP GET이나 POST 요청을 이용하여 질의를 웹 검색엔진 게이트웨이로 보냄
```



##### 9.6.5 검색 결과를 정렬하고 보여주기

```
- 결과를 이용하여 최종 사용자를 위한 결과 페이지를 즉석에서 만들어냄
- 검색엔진은 그 문서들이 주어진 단어와 가장 관련이 많은 순서대로 관련도 랭킹으로 점수를 매기고 정렬
```



##### 9.6.6 스푸핑

```
- 사용자의 검색 결과가 최상위 줄들에 보이지 않으면 불만족스러워함
- 그에 따라 수많은 키워드을 나열한 가짜 페이지를 만들거나, 알고리즘을 더 잘 속일 수 있는 특정 단어에 대한 가짜 페이지를 생성하는 게이트웨이 애플리케이션을 만들어 사용
```





